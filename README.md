<div align="center">

# ğŸ§  FineWeb-Edu LLM Training

**Production-grade QLoRA fine-tuning of Llama-2-13B on educational web content â€” with a RAG-powered chatbot built in.**

[![Model](https://img.shields.io/badge/Model-Llama--2--13B-blueviolet)](https://huggingface.co/NousResearch/Llama-2-13b-hf)
[![Dataset](https://img.shields.io/badge/Dataset-FineWeb--Edu-blue)](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu)
[![GPU](https://img.shields.io/badge/GPU-H100%2080GB-green)](https://www.nvidia.com/en-us/data-center/h100/)
[![License](https://img.shields.io/badge/License-MIT-yellow)](LICENSE)

</div>

---

## ğŸ‘‹ What is this?

This project takes **Meta's Llama-2 13B** model and fine-tunes it on **1 million high-quality educational passages** from the FineWeb-Edu dataset. The result is a language model that's better at explaining concepts, answering questions, and holding educational conversations.

On top of that, there's a **RAG chatbot** (`chat_llm.py`) that doesn't just rely on what the model "remembers" â€” it actively searches a local knowledge base to back up its answers with real passages.

Think of it as a smarter tutor that can both reason *and* look things up.

---

## âœ¨ Key Features

| Feature | Details |
| :--- | :--- |
| ğŸ¦™ **Llama-2-13B** | 13 billion parameter base model from Meta |
| âš¡ **QLoRA** | 4-bit NF4 quantization â€” trains a 13B model on a single GPU |
| ğŸš€ **H100 Optimized** | Flash Attention 2, BF16, TF32, Batch 2 / Accum 4 |
| ğŸ›¡ï¸ **Grad Checkpointing** | CRITICAL: Reduces activation VRAM from 40GB to ~4GB |
| ğŸ“Š **Live Diagnostics** | Real-time it/s, tok/s, and ETA monitoring during training |
| ğŸ“š **1M Samples** | Streamed from FineWeb-Edu (never loads full dataset into RAM) |
| ğŸ” **RAG Chat** | FAISS vector search + live HuggingFace fallback |
| ğŸ’¾ **Auto-Resume** | Checkpoints save to Google Drive; training resumes if Colab disconnects |
| ğŸ§¹ **Memory Cleanup** | Throttled MemoryCallback (every 50 steps) for max throughput |

---

## ğŸ—ï¸ How It Works

```
FineWeb-Edu (1M samples)
        â”‚
        â–¼
   Streaming Tokenizer â”€â”€â–º QLoRA Trainer (H100)
                                  â”‚
                                  â–¼
                          LoRA Adapters (saved to Drive)
                                  â”‚
                                  â–¼
                          RAG Chatbot (Llama-2-13B)
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚ FAISS Index  â”‚ â—„â”€â”€ 100K passages
                         â”‚ (local)      â”‚
                         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                         â”‚ HuggingFace â”‚ â—„â”€â”€ live cloud search
                         â”‚ (fallback)   â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Training Configuration

<table>
  <tr><td><b>Base Model</b></td><td><code>NousResearch/Llama-2-13b-hf</code></td></tr>
  <tr><td><b>Quantization</b></td><td>4-bit NF4 + Double Quantization</td></tr>
  <tr><td><b>LoRA Rank</b></td><td>r=32, alpha=64, bias=none</td></tr>
  <tr><td><b>LoRA Targets</b></td><td><code>q_proj</code>, <code>k_proj</code>, <code>v_proj</code>, <code>o_proj</code></td></tr>
  <tr><td><b>Sequence Length</b></td><td>1,024 tokens (Optimized for VRAM)</td></tr>
  <tr><td><b>Batch Size</b></td><td>2 per device (Gradient Accumulation 4)</td></tr>
  <tr><td><b>Optimizer</b></td><td>AdamW 8-bit (bitsandbytes)</td></tr>
  <tr><td><b>LR Schedule</b></td><td>Cosine (1e-4, 150 warmup steps)</td></tr>
  <tr><td><b>Precision</b></td><td>BFloat16 + TF32</td></tr>
  <tr><td><b>Attention</b></td><td>Flash Attention 2</td></tr>
  <tr><td><b>Grad Checkpointing</b></td><td>Enabled (Required for 13B on 80GB)</td></tr>
  <tr><td><b>Dataloader</b></td><td>4 workers, persistent, pinned, drop_last</td></tr>
  <tr><td><b>Max Steps</b></td><td>5,000</td></tr>
  <tr><td><b>Hardware</b></td><td>NVIDIA H100 80GB HBM3</td></tr>
</table>

**Expected throughput**: ~1.1â€“1.3 it/s on H100 with checkpointing enabled.

---

## ğŸ” Authentication

This project uses the **NousResearch/Llama-2-13b-hf** community mirror, which is **fully open and ungated** â€” no HuggingFace token or license acceptance is required. Just run the notebook and it downloads automatically.

---

## ğŸ“‚ Project Structure

```
fineweb-edu-llm-training/
â”œâ”€â”€ train.ipynb          # Fine-tuning notebook (H100-optimized)
â”œâ”€â”€ chat_llm.py          # Llama-2-13B RAG chatbot
â”œâ”€â”€ build_rag_index.py   # FAISS index builder
â”œâ”€â”€ README.md            # Documentation
â””â”€â”€ out/
    â”œâ”€â”€ final_model/     # LoRA adapters (adapter_config.json, etc.)
    â””â”€â”€ rag_index/       # FAISS index (faiss_index.bin, passages.npy)
```

---

## ğŸš€ Getting Started

### Cloud Training (Recommended)

1. Upload `train.ipynb` to [Google Colab](https://colab.research.google.com)
2. Set the runtime to **H100 GPU**
3. Run all cells â€” hardware diagnostics will confirm your setup
4. Model and RAG index are saved to your Google Drive automatically

### Local Chat

Once you've trained the model and downloaded the files into the `out/` folder:

```bash
# 1. Install dependencies
pip install torch transformers datasets faiss-cpu sentence-transformers peft bitsandbytes accelerate

# 2. Start chatting
python chat_llm.py
```

---

## ğŸ¤ Contributing

Found a bug? Have an idea? Feel free to open an issue or submit a PR. All contributions are welcome.

## ğŸ“ License

This project is licensed under the [MIT License](LICENSE).

---

<div align="center">
  <sub>Built with â¤ï¸ using HuggingFace Transformers, PEFT, and local H100 compute.</sub>
</div>
